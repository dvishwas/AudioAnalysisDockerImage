# Audio Analysis Template - Combined Pyannote + SpeechBrain

Production-ready Docker image combining pyannote speaker diarization and SpeechBrain speaker recognition in a unified FastAPI application. Optimized for GPU acceleration with automatic fallback to CPU.

## üöÄ Quick Start

### Docker Hub
```bash
# Pull and run with GPU support
docker pull dvishwas/audio-analysis-template:latest
docker run --gpus all -p 8000:8000 dvishwas/audio-analysis-template:latest

# CPU-only mode
docker run -p 8000:8000 dvishwas/audio-analysis-template:latest
```

### RunPod Deployment
**Container Image**: `dvishwas/audio-analysis-template:latest`  
**Container Disk**: 20GB minimum (40GB recommended for GPU)  
**GPU**: RTX 3090/4090 or A100 recommended  
**Expose HTTP Ports**: `8000`  
**Docker Command**: `python -m uvicorn main:app --host 0.0.0.0 --port 8000`

### Local Development
```bash
git clone https://github.com/your-username/docker-image-pyannote-speechbrain
cd docker-image-pyannote-speechbrain
docker build --build-arg HF_TOKEN_BUILD=your_hf_token -t audio-analysis .
docker run --gpus all -p 8000:8000 audio-analysis
```

## üìã API Endpoints

### Health Check
**GET** `/health`
```bash
curl https://your-pod-id-8000.proxy.runpod.net/health
```
**Response:**
```json
{
  "status": "healthy",
  "diarization_model_loaded": true,
  "verification_model_loaded": true,
  "device": "cuda",
  "gpu_available": true
}
```

### Speaker Diarization
**POST** `/diarize`

Identifies "who spoke when" in audio files. Returns timestamped speaker segments.

```bash
curl -X POST -F "audio=@meeting.wav" \
  https://your-pod-id-8000.proxy.runpod.net/diarize
```

**Response:**
```json
{
  "diarization_results": [
    {"start": 0.5, "end": 3.2, "speaker": "SPEAKER_00"},
    {"start": 3.5, "end": 7.1, "speaker": "SPEAKER_01"},
    {"start": 7.3, "end": 12.8, "speaker": "SPEAKER_00"}
  ]
}
```

### Speaker Embedding
**POST** `/embedding`

Extracts 192-dimensional speaker embedding vector for voice identification.

```bash
curl -X POST -F "audio=@speaker_sample.wav" \
  https://your-pod-id-8000.proxy.runpod.net/embedding
```

**Response:**
```json
{
  "embedding": [0.123, -0.456, 0.789, ...]
}
```

### Speaker Comparison
**POST** `/compare`

Compares two audio files to determine if they contain the same speaker.

```bash
curl -X POST -F "audio1=@speaker1.wav" -F "audio2=@speaker2.wav" \
  https://your-pod-id-8000.proxy.runpod.net/compare
```

**Response:**
```json
{
  "similarity_score": 0.87,
  "same_speaker": true,
  "threshold": 0.25
}
```

## üéØ Use Cases

- **Meeting Transcription**: Identify speakers in conference calls
- **Voice Authentication**: Verify speaker identity using embeddings
- **Content Analysis**: Analyze podcast/video speaker segments
- **Call Center Analytics**: Track agent-customer interactions
- **Security Applications**: Voice-based access control

## ‚ú® Features

### Core Capabilities
- ‚úÖ **Speaker Diarization** - Who spoke when with precise timestamps
- ‚úÖ **Speaker Recognition** - Extract unique voice embeddings
- ‚úÖ **Speaker Verification** - Compare voices for similarity
- ‚úÖ **Multi-format Support** - WAV, MP3, FLAC, M4A, OGG

### Performance & Reliability
- ‚úÖ **GPU Acceleration** - 5-10x faster processing with CUDA
- ‚úÖ **Automatic Fallback** - CPU mode when GPU unavailable
- ‚úÖ **Memory Optimized** - Efficient model loading and inference
- ‚úÖ **Production Ready** - FastAPI with proper error handling

### Audio Processing
- ‚úÖ **Automatic Resampling** - Handles any sample rate (converts to 16kHz)
- ‚úÖ **Noise Robust** - Works with real-world audio quality
- ‚úÖ **Variable Length** - Processes files from seconds to hours
- ‚úÖ **Batch Processing** - Multiple file support via API

## üîß Technical Specifications

### Models
| Component | Model | Version | Purpose |
|-----------|-------|---------|---------|
| Diarization | pyannote/speaker-diarization-3.1 | 3.1 | Speaker segmentation |
| Recognition | speechbrain/spkrec-ecapa-voxceleb | 0.5.15 | Speaker embeddings |

### System Requirements
| Resource | Minimum | Recommended | Notes |
|----------|---------|-------------|-------|
| RAM | 8GB | 16GB+ | Model loading |
| Storage | 20GB | 40GB+ | Models + cache |
| GPU | GTX 1080 | RTX 3090+ | CUDA 12.1+ |
| CPU | 4 cores | 8+ cores | CPU fallback |

### Performance Benchmarks
| Audio Length | GPU (RTX 3090) | CPU (8-core) | Memory Usage |
|--------------|----------------|--------------|--------------|
| 1 minute | ~5 seconds | ~45 seconds | 4GB |
| 10 minutes | ~25 seconds | ~6 minutes | 6GB |
| 1 hour | ~2.5 minutes | ~35 minutes | 8GB |

## üèóÔ∏è Build Configuration

### Docker Build Arguments
```bash
docker build \
  --build-arg HF_TOKEN_BUILD=your_huggingface_token \
  --platform linux/amd64 \
  -t audio-analysis .
```

### Environment Variables
| Variable | Default | Description |
|----------|---------|-------------|
| `HF_TOKEN_BUILD` | - | HuggingFace token for model download |
| `CUDA_VISIBLE_DEVICES` | all | GPU device selection |
| `PYTORCH_CUDA_ALLOC_CONF` | - | CUDA memory management |

### Dependencies
```dockerfile
# Core ML libraries
torch==2.2.0
torchaudio
torchvision
numpy==1.26.4

# Audio processing
pyannote.audio
speechbrain==0.5.15
soundfile

# API framework
fastapi
uvicorn[standard]
python-multipart
```

## üöÄ Deployment Options

### RunPod (Recommended)
1. **Template Creation**: Use `dvishwas/audio-analysis-template:latest`
2. **GPU Selection**: RTX 3090, RTX 4090, or A100
3. **Storage**: 40GB+ container disk
4. **Ports**: Expose 8000
5. **Command**: `python -m uvicorn main:app --host 0.0.0.0 --port 8000`

### AWS ECS/Fargate
```yaml
# task-definition.json
{
  "family": "audio-analysis",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "2048",
  "memory": "8192",
  "containerDefinitions": [{
    "name": "audio-analysis",
    "image": "dvishwas/audio-analysis-template:latest",
    "portMappings": [{"containerPort": 8000}]
  }]
}
```

### Kubernetes
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audio-analysis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: audio-analysis
  template:
    metadata:
      labels:
        app: audio-analysis
    spec:
      containers:
      - name: audio-analysis
        image: dvishwas/audio-analysis-template:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            nvidia.com/gpu: 1
```

## üîç Troubleshooting

### Common Issues

**GPU Not Detected**
```bash
# Check CUDA availability
docker exec -it container_name python -c "import torch; print(torch.cuda.is_available())"

# Verify GPU access
docker exec -it container_name nvidia-smi
```

**High Memory Usage**
- Reduce batch size for long audio files
- Use CPU mode for memory-constrained environments
- Monitor with `docker stats container_name`

**Slow Processing**
- Ensure GPU drivers are installed
- Check CUDA version compatibility
- Verify model loading on correct device

### Performance Optimization

**GPU Memory Management**
```python
# Set memory fraction
import torch
torch.cuda.set_per_process_memory_fraction(0.8)
```

**Audio Preprocessing**
- Convert to WAV format beforehand
- Resample to 16kHz for optimal performance
- Trim silence to reduce processing time

## üìä API Response Formats

### Error Responses
```json
{
  "detail": "Error message",
  "error_type": "ValidationError|ProcessingError|ModelError",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### Success Responses
All successful responses include:
- `processing_time`: Time taken in seconds
- `audio_duration`: Input audio length
- `model_device`: Device used (cuda/cpu)

## üîê Security Considerations

- No authentication required (suitable for internal networks)
- File uploads are temporary and auto-deleted
- No persistent storage of audio data
- Models are pre-downloaded (no runtime internet access needed)

## üìà Monitoring & Logging

### Health Monitoring
```bash
# Continuous health check
watch -n 5 'curl -s http://localhost:8000/health | jq'
```

### Performance Metrics
- Processing time per request
- GPU utilization
- Memory usage
- Queue depth (for multiple requests)

## ü§ù Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/new-feature`
3. Commit changes: `git commit -am 'Add new feature'`
4. Push to branch: `git push origin feature/new-feature`
5. Submit pull request

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Pyannote Team](https://github.com/pyannote/pyannote-audio) for speaker diarization
- [SpeechBrain Team](https://speechbrain.github.io/) for speaker recognition
- [RunPod](https://runpod.io/) for GPU infrastructure
