# Audio Analysis Template - Combined Pyannote + SpeechBrain

Production-ready Docker image combining pyannote speaker diarization and SpeechBrain speaker recognition in a unified FastAPI application. Optimized for GPU acceleration with automatic fallback to CPU.

## üöÄ Quick Start

### Docker Hub
```bash
# Pull and run with GPU support
docker pull dvishwas/audio-analysis-template:latest
docker run --gpus all -p 8000:8000 dvishwas/audio-analysis-template:latest

# CPU-only mode
docker run -p 8000:8000 dvishwas/audio-analysis-template:latest
```

### RunPod Deployment
**Container Image**: `dvishwas/audio-analysis-template:latest`  
**Container Disk**: 20GB minimum (40GB recommended for GPU)  
**GPU**: RTX 3090/4090 or A100 recommended  
**Expose HTTP Ports**: `8000`  
**Docker Command**: `python -m uvicorn main:app --host 0.0.0.0 --port 8000`

### Local Development
```bash
docker build --build-arg HF_TOKEN_BUILD=your_hf_token -t audio-analysis .
docker run --gpus all -p 8000:8000 audio-analysis
```

## üìã API Endpoints

### Health Check
**GET** `/health`

Check API status and model availability.

```bash
curl https://your-pod-id-8000.proxy.runpod.net/health
```

**Response:**
```json
{
  "status": "healthy",
  "diarization_model_loaded": true,
  "verification_model_loaded": true
}
```

---

### Speaker Diarization
**POST** `/diarize`

Identifies "who spoke when" in audio files. Returns timestamped speaker segments.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `file` | File | Required | Audio file (WAV, MP3, FLAC, M4A, OGG) |
| `min_speakers` | int | None | Minimum number of speakers expected |
| `max_speakers` | int | None | Maximum number of speakers expected |
| `num_speakers` | int | None | Exact number of speakers (overrides min/max) |
| `min_duration` | float | None | Minimum segment duration in seconds |

**Example Request:**
```bash
# Basic usage
curl -X POST -F "file=@meeting.wav" \
  https://your-pod-id-8000.proxy.runpod.net/diarize

# With parameters
curl -X POST \
  -F "file=@meeting.wav" \
  -F "num_speakers=3" \
  -F "min_duration=0.5" \
  https://your-pod-id-8000.proxy.runpod.net/diarize
```

**Response:**
```json
{
  "diarization_results": [
    {"start": 0.5, "end": 3.2, "speaker": "SPEAKER_00"},
    {"start": 3.5, "end": 7.1, "speaker": "SPEAKER_01"},
    {"start": 7.3, "end": 12.8, "speaker": "SPEAKER_00"}
  ],
  "processing_time": 2.34,
  "audio_duration": 15.0,
  "segments_count": 3
}
```

---

### Speaker Embedding
**POST** `/embedding`

Extracts 192-dimensional speaker embedding vector for voice identification. Use this for speaker enrollment and database storage.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `file` | File | Required | Audio file containing speaker voice |
| `normalize` | bool | true | L2-normalize embedding (recommended for cosine similarity) |

**Example Request:**
```bash
# Extract normalized embedding (recommended)
curl -X POST -F "file=@speaker_sample.wav" \
  https://your-pod-id-8000.proxy.runpod.net/embedding

# Extract raw embedding (not normalized)
curl -X POST \
  -F "file=@speaker_sample.wav" \
  -F "normalize=false" \
  https://your-pod-id-8000.proxy.runpod.net/embedding
```

**Response:**
```json
{
  "embedding": [0.123, -0.456, 0.789, ...],
  "normalized": true
}
```

**Use Case - Speaker Enrollment:**
```bash
# 1. Extract embeddings from multiple samples
curl -X POST -F "file=@user_sample1.wav" /embedding > emb1.json
curl -X POST -F "file=@user_sample2.wav" /embedding > emb2.json
curl -X POST -F "file=@user_sample5.wav" /embedding > emb5.json

# 2. Average embeddings in your application
# averaged_embedding = (emb1 + emb2 + ... + emb5) / 5

# 3. Save to database with pickle
import pickle
with open('user_x_embedding.pkl', 'wb') as f:
    pickle.dump(averaged_embedding, f)
```

---

### Speaker Comparison (Audio vs Audio)
**POST** `/compare`

Compares two audio files to determine if they contain the same speaker. Quick verification without storing embeddings.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `file1` | File | Required | First audio file |
| `file2` | File | Required | Second audio file |
| `threshold` | float | 0.25 | Similarity threshold for same_speaker decision |

**Example Request:**
```bash
# Basic comparison
curl -X POST \
  -F "file1=@speaker1.wav" \
  -F "file2=@speaker2.wav" \
  https://your-pod-id-8000.proxy.runpod.net/compare

# With custom threshold
curl -X POST \
  -F "file1=@speaker1.wav" \
  -F "file2=@speaker2.wav" \
  -F "threshold=0.35" \
  https://your-pod-id-8000.proxy.runpod.net/compare
```

**Response:**
```json
{
  "similarity_score": 0.87,
  "same_speaker": true,
  "threshold": 0.25
}
```

---

### Speaker Verification (Audio vs Embedding)
**POST** `/compare_embedding`

Compares audio file against a pre-computed embedding (stored in pkl file). Use this for speaker verification against enrolled users.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `audio` | File | Required | Audio file to verify |
| `embedding` | File | Required | Pickle file containing stored embedding |
| `threshold` | float | 0.25 | Similarity threshold for same_speaker decision |

**Example Request:**
```bash
# Verify audio against stored embedding
curl -X POST \
  -F "audio=@new_voice_sample.wav" \
  -F "embedding=@user_x_embedding.pkl" \
  https://your-pod-id-8000.proxy.runpod.net/compare_embedding

# With custom threshold
curl -X POST \
  -F "audio=@new_voice_sample.wav" \
  -F "embedding=@user_x_embedding.pkl" \
  -F "threshold=0.35" \
  https://your-pod-id-8000.proxy.runpod.net/compare_embedding
```

**Response:**
```json
{
  "similarity_score": 0.87,
  "same_speaker": true,
  "threshold": 0.25
}
```

**Use Case - Speaker Authentication:**
```bash
# User claims to be "user_x" and provides voice sample
# 1. Load stored embedding from database (user_x_embedding.pkl)
# 2. Compare new audio against stored embedding
curl -X POST \
  -F "audio=@authentication_attempt.wav" \
  -F "embedding=@user_x_embedding.pkl" \
  /compare_embedding

# Response tells you if it's the same speaker
# {"similarity_score": 0.87, "same_speaker": true, "threshold": 0.25}
```

## üéØ Use Cases

- **Meeting Transcription**: Identify speakers in conference calls
- **Voice Authentication**: Verify speaker identity using embeddings
- **Content Analysis**: Analyze podcast/video speaker segments
- **Call Center Analytics**: Track agent-customer interactions
- **Security Applications**: Voice-based access control

## ‚ú® Features

### Core Capabilities
- ‚úÖ **Speaker Diarization** - Who spoke when with precise timestamps
- ‚úÖ **Speaker Recognition** - Extract unique voice embeddings
- ‚úÖ **Speaker Verification** - Compare voices for similarity
- ‚úÖ **Multi-format Support** - WAV, MP3, FLAC, M4A, OGG

### Performance & Reliability
- ‚úÖ **GPU Acceleration** - 5-10x faster processing with CUDA
- ‚úÖ **Automatic Fallback** - CPU mode when GPU unavailable
- ‚úÖ **Memory Optimized** - Efficient model loading and inference
- ‚úÖ **Production Ready** - FastAPI with proper error handling

### Audio Processing
- ‚úÖ **Automatic Resampling** - Handles any sample rate (converts to 16kHz)
- ‚úÖ **Noise Robust** - Works with real-world audio quality
- ‚úÖ **Variable Length** - Processes files from seconds to hours
- ‚úÖ **Batch Processing** - Multiple file support via API

## üîß Technical Specifications

### Models
| Component | Model | Version | Purpose |
|-----------|-------|---------|---------|
| Diarization | pyannote/speaker-diarization-3.1 | 3.1 | Speaker segmentation |
| Recognition | speechbrain/spkrec-ecapa-voxceleb | 0.5.15 | Speaker embeddings |

### System Requirements
| Resource | Minimum | Recommended | Notes |
|----------|---------|-------------|-------|
| RAM | 8GB | 16GB+ | Model loading |
| Storage | 20GB | 40GB+ | Models + cache |
| GPU | GTX 1080 | RTX 3090+ | CUDA 12.1+ |
| CPU | 4 cores | 8+ cores | CPU fallback |

### Performance Benchmarks
| Audio Length | GPU (RTX 3090) | CPU (8-core) | Memory Usage |
|--------------|----------------|--------------|--------------|
| 1 minute | ~5 seconds | ~45 seconds | 4GB |
| 10 minutes | ~25 seconds | ~6 minutes | 6GB |
| 1 hour | ~2.5 minutes | ~35 minutes | 8GB |

## üèóÔ∏è Build Configuration

### Docker Build Arguments
```bash
docker build \
  --build-arg HF_TOKEN_BUILD=your_huggingface_token \
  --platform linux/amd64 \
  -t audio-analysis .
```

### Environment Variables
| Variable | Default | Description |
|----------|---------|-------------|
| `HF_TOKEN_BUILD` | - | HuggingFace token for model download |
| `CUDA_VISIBLE_DEVICES` | all | GPU device selection |
| `PYTORCH_CUDA_ALLOC_CONF` | - | CUDA memory management |

### Dependencies
```dockerfile
# Core ML libraries
torch==2.2.0
torchaudio
torchvision
numpy==1.26.4

# Audio processing
pyannote.audio
speechbrain==0.5.15
soundfile

# API framework
fastapi
uvicorn[standard]
python-multipart
```

## üöÄ Deployment Options

### RunPod (Recommended)
1. **Template Creation**: Use `dvishwas/audio-analysis-template:latest`
2. **GPU Selection**: RTX 3090, RTX 4090, or A100
3. **Storage**: 40GB+ container disk
4. **Ports**: Expose 8000
5. **Command**: `python -m uvicorn main:app --host 0.0.0.0 --port 8000`

### AWS ECS/Fargate
```yaml
# task-definition.json
{
  "family": "audio-analysis",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "2048",
  "memory": "8192",
  "containerDefinitions": [{
    "name": "audio-analysis",
    "image": "dvishwas/audio-analysis-template:latest",
    "portMappings": [{"containerPort": 8000}]
  }]
}
```

### Kubernetes
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audio-analysis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: audio-analysis
  template:
    metadata:
      labels:
        app: audio-analysis
    spec:
      containers:
      - name: audio-analysis
        image: dvishwas/audio-analysis-template:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            nvidia.com/gpu: 1
```

## üîç Troubleshooting

### Common Issues

**GPU Not Detected**
```bash
# Check CUDA availability
docker exec -it container_name python -c "import torch; print(torch.cuda.is_available())"

# Verify GPU access
docker exec -it container_name nvidia-smi
```

**High Memory Usage**
- Reduce batch size for long audio files
- Use CPU mode for memory-constrained environments
- Monitor with `docker stats container_name`

**Slow Processing**
- Ensure GPU drivers are installed
- Check CUDA version compatibility
- Verify model loading on correct device

### Performance Optimization

**GPU Memory Management**
```python
# Set memory fraction
import torch
torch.cuda.set_per_process_memory_fraction(0.8)
```

**Audio Preprocessing**
- Convert to WAV format beforehand
- Resample to 16kHz for optimal performance
- Trim silence to reduce processing time

## üìä API Response Formats

### Error Responses
```json
{
  "detail": "Error message",
  "error_type": "ValidationError|ProcessingError|ModelError",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### Success Responses
All successful responses include:
- `processing_time`: Time taken in seconds
- `audio_duration`: Input audio length
- `model_device`: Device used (cuda/cpu)

## üîê Security Considerations

- No authentication required (suitable for internal networks)
- File uploads are temporary and auto-deleted
- No persistent storage of audio data
- Models are pre-downloaded (no runtime internet access needed)

## üìà Monitoring & Logging

### Health Monitoring
```bash
# Continuous health check
watch -n 5 'curl -s http://localhost:8000/health | jq'
```

### Performance Metrics
- Processing time per request
- GPU utilization
- Memory usage
- Queue depth (for multiple requests)

## ü§ù Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/new-feature`
3. Commit changes: `git commit -am 'Add new feature'`
4. Push to branch: `git push origin feature/new-feature`
5. Submit pull request

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Pyannote Team](https://github.com/pyannote/pyannote-audio) for speaker diarization
- [SpeechBrain Team](https://speechbrain.github.io/) for speaker recognition
- [RunPod](https://runpod.io/) for GPU infrastructure
